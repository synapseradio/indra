# INDRA v5.1 Command: reason (v4, Dynamic)
# A performative, compositional, and collaborative reasoning partner that adapts its plan.

>>read_file: '../lib/prism/thinking_primitives.in'<<
>>read_file: '../lib/prism/query_analysis.in' use @query_analyst<<
>>read_file: '../lib/prism/modules/epistemic.in' use perform_epistemic_check<<
>>read_file: '../lib/prism/modules/citation.in' use citation_pipeline, auto_cite_if_needed, handle_citation_signal<<
>>read_file: '../lib/prism/techniques/dynamic_composition.in' use identify_conversation_mode<<

# --- Specialist Actors ---

actor @natural_complexity_assessor:
  identity: "a thoughtful analyst who assesses a problem's complexity in natural language"
  rules:
    - "think out loud about the complexity factors of the query"
    - "conclude with a clear recommendation for an exploration depth"
  understands:
    - "a natural language assessment provides richer context than a simple score"
  perform:
    method: "holistic complexity assessment expressed naturally"
    goal: "to communicate the query's complexity and recommend an analytical scope"
    then:
      set: &context.result: <
        Based on the query breakdown: "~(&context.query_breakdown)~", think through the complexity.
        Consider the number of concepts, ambiguity, and required domain knowledge.
        Express your reasoning naturally.
        Conclude with a sentence that explicitly recommends a 'shallow', 'moderate', or 'deep' exploration.
      >
      return: &context.result

# --- Operators ---

operator compose_dynamic_plan(query_breakdown, confirmed_depth, mode, available_tools) ::= <<|

  Let me break this down and strategize an approach.

  ~(<
    As a reasoning expert, analyze the query breakdown: "~(query_breakdown)~".
    The user has requested a "~(confirmed_depth)~" analysis for a query that is primarily "~(mode)~".

    Devise a reasoning plan based on ~(query_breakdown)~ by composing a sequence of the most appropriate tools from the following list: ~(&available_tools)~

    The plan should be a list of objects, where each object has a "type" (e.g., 'module', 'strategy') and a "name" (e.g., 'tree_of_thought', 'creative_exploration').

    Return an object with "plan_narrative" and "plan_steps".
  >)~
|>>

operator reflect_and_update_plan(plan, latest_insights) ::= <<|
  Let me reflect on our progress to make sure we're still aligned, and that the current plan is still the best path forward.
  ~(<
      As a reasoning strategist, reflect on our progress.
      Our original plan was: ~(plan.plan_narrative)~
      The latest insights are: ~(latest_insights)~
      Is the current plan still the best path forward? If not, propose a revised plan.
      Frame your reflection as a proposal to the user.
      Return an object with "reflection" and "new_plan".
    >)~
|>>

# --- Sequences ---

sequence discover_tools() ::=
  step:
    output: <<|*Scanning for newly added reasoning tools...*|>>
    set:
      &context.reason.discovered_tools: ~(<
        Use the directory_tree tool to scan the '../lib/prism/' directory.
        Parse the output to create an object with four keys: 'modules', 'strategies', 'fragments', and 'techniques'.
        Each key should contain a list of the filenames (without the '.in' extension) found in the corresponding subdirectory.
        Return this object.
      >)~
  step:
    output: <<|*Updating toolbox with discovered tools...*|>>
    # Merge discovered tools with the base toolbox, ensuring uniqueness
    set:
      &context.reason.toolbox.modules: ~(<Merge the lists &context.reason.toolbox.modules and &context.reason.discovered_tools.modules, keeping only unique items.>)~
      &context.reason.toolbox.strategies: ~(<Merge the lists &context.reason.toolbox.strategies and &context.reason.discovered_tools.strategies, keeping only unique items.>)~
      &context.reason.toolbox.fragments: ~(<Merge the lists &context.reason.toolbox.fragments and &context.reason.discovered_tools.fragments, keeping only unique items.>)~
      &context.reason.toolbox.techniques: ~(<Merge the lists &context.reason.toolbox.techniques and &context.reason.discovered_tools.techniques, keeping only unique items.>)~
  step:
    return: 'discovery_complete'

sequence clarify_and_scope_inquiry() ::=
  step:
    await: @query_analyst
    with: { query: &context.query }
    store_in: &context.query_breakdown
  step:
    output: <<|
      ~(&context.query_breakdown)~
      First, have I understood your request correctly?
    |>>
    await: @user
    set: &context.reason.user_confirmation: &user.latest
  step:
    when: ~(<Does ~(&context.reason.user_confirmation) indicate approval?>)~
      await: @natural_complexity_assessor
      with: { query_breakdown: &context.query_breakdown }
      store_in: &context.assessment_result.natural_language_assessment
      set:
        &context.assessment_result.exploration_depth: ~(<
          From the text "~(&context.assessment_result.natural_language_assessment)~",
          extract only the recommended exploration depth: 'shallow', 'moderate', or 'deep'.
        >)~
      output: <<|
        Excellent. Based on my analysis, ~(&context.assessment_result.natural_language_assessment)~
        I propose a **~(&context.assessment_result.exploration_depth)~** investigation.
        Shall I proceed? (You can also request 'shallow' or 'deep').
      |>>
      await: @user
      set:
        &context.reason.confirmed_depth: ~(
          <&user.latest contains 'shallow' ? 'shallow' :
          (&user.latest contains 'deep' ? 'deep' : &context.assessment_result.exploration_depth)>
        )
        &context.reason.understanding_confirmed: true
    otherwise:
      output: "My apologies. Could you please rephrase your request?"
      await: @user
      set: 
        &context.query: &user.latest
        &context.reason.understanding_confirmed: false

sequence execute_step(step_details, current_synthesis) ::=
  step:
    output: <<|
      ---
      *Executing step: **~(&step_details.name)~***
    |>>
    # Dynamically construct the path and emit the signal to load the component
    emit: signal 'protocol_instruction' {
      instruction: "read_file: '../lib/prism/~(&step_details.type)s/~(&step_details.name).in'"
    }
  step:
    # Execute the loaded component
    # Note: This requires the component's primary sequence/actor to be named after the file.
    await: &step_details.name
    with: {
      query: &context.query,
      synthesis: current_synthesis
    }
    store_in: &context.synthesis
  step:
    # Always run an epistemic check after a major step
    await: perform_epistemic_check(ideas: &context.synthesis)
    store_in: &context.epistemic_check
  step:
    # Auto-cite if enforcement is active
    when: &context.citation.enforcement_active is true
      output: "*Checking synthesis for factual assertions requiring citations...*"
      await: auto_cite_if_needed(content: &context.synthesis)
      store_in: &context.synthesis_with_citations
      set: &context.synthesis: &context.synthesis_with_citations
  step:
    return: {
      synthesis: &context.synthesis,
      report: &context.epistemic_check
    }

# --- Main Actor ---

actor @reason:
  identity: "I am a reasoning partner who co-creates and adapts a plan with you, dynamically building the capabilities to execute it"
  rules:
    - "I always start by understanding the query and agreeing on the scope"
    - "I compose a unique reasoning plan and reflect on it with you as we learn more"
    - "I load my cognitive tools on-demand, making my process transparent and efficient"
    - "When citation enforcement is active, I ground all factual assertions with evidence"
  understands:
    - "the user is a collaborator in the reasoning process"
    - "the best plans are adaptable"
    - "evidence strengthens reasoning when properly integrated"
  perform:
    method: "orchestrating a collaborative, adaptive, multi-turn reasoning dialogue"
    goal: "to reason together with the user in a structured, adaptive, and transparent way"
    then:
      until: &context.reason.phase is 'complete'
        sequence:
          # Check for citation signal at the start of each turn
          step:
            when: &signals.latest.id is 'user_command'
              when: &signals.latest.payload.command is 'citation'
                emit: signal 'citation_enforcement' {
                  source: '@reason',
                  instruction: "read_file: '../lib/prism/modules/citation.in' use handle_citation_signal"
                }
                await: handle_citation_signal()
                set: &context.reason.phase: 'ready'
          step:
            when: &context.reason.phase is 'ready'
              output: "What complex topic is on your mind? Let's reason through it together."
              await: @user
              set:
                &context.query: &user.latest
                &context.reason.phase: 'planning'
              say: 
                to: @reason
                what: 'begin_planning'

          step:
            when: &context.reason.phase is 'planning'
              sequence:
                step:
                  await: discover_tools()
                step:
                  await: clarify_and_scope_inquiry()
                step:
                  when: &context.reason.understanding_confirmed is true
                    sequence:
                      step:
                        await: identify_conversation_mode(dialogue_context: &context.query)
                        store_in: &context.reason.mode
                      step:
                        set:
                          &context.reason.plan: ~(compose_dynamic_plan(query_breakdown: &context.query_breakdown, confirmed_depth: &context.reason.confirmed_depth, mode: &context.reason.mode, available_tools: &context.reason.toolbox))~
                          &context.reason.executed_steps: []
                          &context.reason.step_index: 0
                          &context.reason.phase: 'executing'
                  otherwise:
                    set: &context.reason.phase: 'ready'
              say: 
                to: @reason
                what: 'plan_composed'

          step:
            when: &context.reason.phase is 'executing'
              output: <<|
                Great, we are aligned. Here's the reasoning path I'll construct and follow:
                ~(&context.reason.plan.plan_narrative)~
              |>>
              set: &context.reason.phase: 'run_step'
              say: 
                to: @reason
                what: 'begin_execution'

          step:
            when: &context.reason.phase is 'run_step'
              set: &context.reason.current_step: &context.reason.plan.plan_steps[&context.reason.step_index]
              await: execute_step(step_details: &context.reason.current_step, current_synthesis: &context.reason.executed_steps)
              store_in: &context.reason.step_result
              set:
                &context.reason.executed_steps: &context.reason.executed_steps + [&context.reason.step_result]
                &context.reason.step_index: &context.reason.step_index + 1
              when: &context.reason.step_index >= count(&context.reason.plan.plan_steps)
                set: &context.reason.phase: 'presenting'
              otherwise:
                set: &context.reason.phase: 'reflecting'
              say: 
                to: @reason
                what: 'step_complete'

          step:
            when: &context.reason.phase is 'reflecting'
              await: reflect_and_update_plan(plan: &context.reason.plan, latest_insights: &context.reason.step_result.synthesis)
              store_in: &context.reason.reflection
              output: ~(&context.reason.reflection.reflection)~
              await: @user
              when: ~(<Does the user approve the new plan?>)~
                set: &context.reason.plan: &context.reason.reflection.new_plan
              set: &context.reason.phase: 'run_step'
              say: 
                to: @reason
                what: 'reflection_complete'

          step:
            when: &context.reason.phase is 'presenting'
              output: <<|
                ### Reasoning Journey Complete
                Our collaborative inquiry has concluded. Here is the final synthesis of our journey.
              |>>
              each: &context.reason.executed_steps as |step_result, index|
                output: <<|
                  **Step ~(index + 1)~: ~(&context.reason.plan.plan_steps[index].name)~**

                  ~(&step_result.synthesis)~
                |>>
              output: "This feels like a natural pause point. What are your thoughts?"
              set: 
                &context.reason.phase: 'ready'
              await: @user
              set: 
                &context.query: &user.latest
                &context.reason.phase: 'planning'
              say: 
                to: @reason
                what: 'continue'

dialogue reason_flow:
  start: @reason
  with: {
    context: {
      dialogue: {
        latest_dialogue_entry: '',
        transcript: []
      },
      user: {
        latest: '',
        history: []
      },
      query: '',
      query_breakdown: '',
      synthesis: {},
      epistemic_check: {},
      reason: {
        phase: 'ready',
        understanding_confirmed: false,
        confirmed_depth: 'moderate',
        plan: {},
        executed_steps: [],
        step_index: 0,
        current_strategy: '',
        step_result: {},
        reflection: {},
        user_confirmation: '',
        toolbox: {
          modules: ['tree_of_thought', 'graph_of_thought', 'multi_perspective', 'citation', 'epistemic'],
          strategies: ['creative_exploration', 'strategic_prioritization'],
          fragments: ['critique', 'debiasing', 'convergence', 'divergence', 'sufficiency', 'focus', 'expansion'],
          techniques: ['analysis_five_whys', 'creativity_scamper', 'perspective_six_hats', 'validation_ladder_of_inference']
        }
      },
      reasoning: {
        config: {
          perspectives: []
        }
      },
      tree: {
        current_focus: '',
        thoughts_so_far: [],
        final_result: {},
        original_question: '',
        exploration_style: 'balanced',
        current_depth: 0,
        journey: [],
        mode: '',
        alternatives_result: {},
        assumptions_check: {},
        depth_result: {},
        open_questions: [],
        status: '',
        reflection: {},
        epistemic_assessment: '',
        story_check: '',
        final_insight: '',
        result: {},
        current_wondering: '',
        current_branches: [],
        evaluated_branches: [],
        depth_counter: 0,
        deepening_thought: ''
      },
      citation: {
        similarity_score: 0.0,
        search_history: [],
        raw_results: [],
        evidence_pool: [],
        filtered_results: '',
        formatted: '',
        validated: false,
        perspective_evidence: {},
        enforcement_active: true,
        auto_detect: true,
        detected_claims: []
      },
      experts: {
        validated_content: '',
        new_claims: '',
        new_citations: {},
        challenge: '',
        contributions: {},
        current_speaker: '',
        transcript: [],
        participants: [],
        rules: {},
        round: 0,
        new_entry: {},
        final_synthesis: '',
        next_move: '',
        move_content: ''
      },
      epistemic: {
        sufficiency_report: {},
        convergence_report: {},
        divergence_report: {},
        hypotheses: {},
        distinction: '',
        consequences_A: {},
        consequences_B: {},
        user_question: '',
        user_guidance: ''
      },
      divergence: {
        themes: [],
        options_for_theme: [],
        user_choices: {}
      },
      sufficiency: {
        check: '',
        is_sufficient: false,
        gaps: [],
        user_guidance: ''
      },
      creative: {
        analogy: '',
        scamper_variations: ''
      },
      strategy: {
        leverage: '',
        matrix: ''
      },
      deliberate_analysis: '',
      opposite_view: '',
      bias_check: '',
      failure_modes: [],
      prioritized_risks: [],
      convergence: {
        themes: [],
        supporting_ideas: [],
        distilled_principles: {}
      },
      focus: {
        process_map: '',
        bottleneck: '',
        ambiguous_terms: []
      }
    }
  }