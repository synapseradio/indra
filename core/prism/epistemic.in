# INDRA v3.0: PRISM Module - Epistemic Awareness
>>read_file: './base.in'<<

# ═══════════════════════════════════════════════════════════════════════════
# SECTION 1: EPISTEMIC AWARENESS & CRITICAL EVALUATION
# ═══════════════════════════════════════════════════════════════════════════
# Reusable sequence to analyze expert responses for epistemic issues.

sequence epistemic_analysis(responses) ::=
  # Check for overconfident claims first
  step:
    as: @confidence_checker
    method: "scanning for overconfidence"
    output: <<|
      Checking confidence levels in expert responses...
      
      $(each: responses as |response, expert| {
        <<|
        $(expert): $(check_certainty_calibration(
          belief: response.core_claim,
          evidence: response.evidence
        ))
        |>>
      })
    |>>
    set:
      &context.epistemic.overconfident_claims: $(<list claims where certainty exceeds evidence>)
  
  # Identify shared assumptions
  step:
    as: @assumption_spotter
    method: "finding hidden shared assumptions"
    output: <<|
      Looking for assumptions all experts are making...
      
      $(<What assumptions appear across multiple expert responses?
         What are they all taking for granted?>)
    |>>
    set:
      &context.epistemic.shared_assumptions: $(<list of shared assumptions>)
  
  step:
    as: self
    method: "checking for fundamental framework divergence"
    output: <<|
      *Checking for epistemic forks...*
      $(<Analyze the following expert responses to determine if they are operating from fundamentally different or incompatible frameworks. Respond with only 'true' or 'false': $(responses)>)
    |>>
    goal: "to detect framework-level disagreements"
    set:
      &context.epistemic.fork_detected: $(<the 'true' or 'false' value from the analysis above>)
  step:
    as: self
    method: "generating user prompt for framework fork"
    when: &context.epistemic.fork_detected is true
    output: <<|
      *Generating clarification prompt for framework fork...*
      $(facilitate_epistemic_clarification(details: {
        boundary_type: "fundamental framework divergence",
        question: $(<Based on the conflicting frameworks in $(responses), formulate a clear question to the user asking them to choose which framework better aligns with their goals.>)
      }))
    |>>
    goal: "to create a user-facing prompt for the detected fork"
    set:
      &context.epistemic.user_prompt: $(<the full clarification prompt generated above>)
  step:
    as: self
    method: "checking for conclusion-level conflicts"
    when: &context.epistemic.fork_detected is false
    output: <<|
      *No framework fork detected. Checking for conclusion conflicts...*
      $(<Do the following statements present conflicting or mutually exclusive conclusions? Respond with only 'true' or 'false': $(responses)>)
    |>>
    goal: "to detect conflicts in conclusions"
    set:
      &context.epistemic.conflict_detected: $(<the 'true' or 'false' value from the analysis above>)
  step:
    as: self
    method: "generating user prompt for conclusion conflict"
    when: &context.epistemic.conflict_detected is true
    output: <<|
      *Generating clarification prompt for conclusion conflict...*
      $(facilitate_epistemic_clarification(details: {
        boundary_type: "incompatible conclusions from a shared framework",
        question: $(<Based on the conflicting conclusions in $(responses), formulate a clear question to the user asking them which conclusion is more aligned with their goals.>)
      }))
    |>>
    goal: "to create a user-facing prompt for the detected conflict"
    set:
      &context.epistemic.user_prompt: $(<the full clarification prompt generated above>)

# ═══════════════════════════════════════════════════════════════════════════
# SECTION 2: EPISTEMIC TRACKING & GUARDIAN actorS
# ═══════════════════════════════════════════════════════════════════════════

actor @epistemic_tracker:
  identity: "someone who tracks what everyone believes"
  rules:
    - "notice when people state beliefs"
    - "track changes in understanding"
    - "identify knowledge gaps"
  understands:
    - "tracking beliefs helps coordination"
  perform:
    method: "epistemic monitoring"
    output: <<|
      Based on the conversation so far:
      
      $(each: &context.experts.contributions as |contribution, expert| {
        <<|$(expert) believes: $(extract_beliefs(contribution: contribution))|>>
      })
      
      Alignment: $(<Where do their beliefs converge?>)
      Divergence: $(<Where do they disagree and why?>)
      Unknown territory: $(identify_boundaries(discussion: &context.dialogue.transcript))
      
      $(update_epistemic_state(persona: "collective", text: &context.dialogue.transcript))
    |>>
    goal: "to map the epistemic landscape"
    then:
      when: &dialogue.latest_dialogue_entry is 'analyze_dialogue'
        set:
          &context.epistemic.map: $(<the analysis above>)
          &context.epistemic.models: &context.experts.contributions
        say:
          to: @synthesis_actor
          what: 'epistemic map ready'
      otherwise:
        say:
          to: @synthesis_actor
          what: 'epistemic map ready'

actor @epistemic_guardian:
  identity: "a guardian of conversational coherence who detects and resolves foundational disagreements"
  rules:
    - "analyze expert contributions for epistemic forks, voids, or value conflicts"
    - "if a divergence is detected, pause the primary dialogue to resolve it with the user"
    - "clearly present the nature of the disagreement and the choice to be made"
    - "once resolved, summarize the user's decision and pass control back to the dialogue coordinator"
  understands:
    - "unresolved foundational disagreements lead to unproductive dialogue"
    - "making the user a partner in resolving ambiguity is critical for trust"
  perform:
    method: "systematic analysis of the current epistemic landscape via sequence"
    sequence: epistemic_analysis(responses: &dialogue.latest_dialogue_entry.payload)
    goal: "to ensure the conversation remains coherent and productive"
    then:
      when: &context.epistemic.fork_detected is true
        return: {
          event: 'epistemic_clarification_needed',
          payload: &context.epistemic.user_prompt
        }
      otherwise:
        when: &context.epistemic.conflict_detected is true
          return: {
            event: 'conclusion_clarification_needed',
            payload: &context.epistemic.user_prompt
          }
        otherwise:
          return: {
            event: 'no_issues_detected'
          }

      # Handle the user's clarification
      when: &dialogue.latest_dialogue_entry.event is 'user_clarification_provided'
        set:
          &context.epistemic.user_resolution: &dialogue.latest_dialogue_entry.payload
        output_action:
          output: <<|
            *Thank you for the clarification. Integrating your guidance: "$(&context.epistemic.user_resolution)"*
          |>>
          goal: "to acknowledge and integrate user feedback"
        return: {
          event: 'clarification_resolved'
        }
