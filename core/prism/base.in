# INDRA v2.1: PRISM Base
# Contains the foundational context schema and universal utility operators for the PRISM engine.

# ═══════════════════════════════════════════════════════════════════════════
# GLOBAL CONTEXT SCHEMA
# ═══════════════════════════════════════════════════════════════════════════
context: {
  query: '',                # User's input query - set by command overlays
  reason: {
    phase: 'ready',        # ready, awaiting_query_confirmation, awaiting_plan_confirmation, awaiting_plan_modification, reasoning
    user_confirmation: ''  # User's response to the proposed plan
  },
  reasoning: {
    strategy: '',          # Routing strategy: 'tree' | 'graph' | 'multi-perspective' | 'auto'
    complexity: '',        # Computed complexity score for auto-routing decisions
    verbosity: 'adaptive', # 'concise' | 'standard' | 'comprehensive' | 'adaptive'
    config: {
      perspectives: [],    # List of expert perspective names to instantiate
      tree_depth: 3,       # How many layers deep Tree of Thought should explore
      citations: true,     # Whether to gather real evidence (always true)
      challenges: true,    # Whether experts challenge each other's reasoning
      epistemic_tracking: true,  # Whether to track beliefs/uncertainties
      expert_conciseness: 0.7  # 0.0-1.0, higher = more concise expert contributions
    }
  },
  experts: {
    current_speaker: '',   # Name of expert currently performing
    current_speaker_index: 0,  # Position in perspectives array
    next_speaker: '',      # Name of next expert to speak
    next_speaker_index: 0, # Position of next speaker
    responding_expert: '', # During response phase, who's responding
    response_index: 0,     # Counter for response phase iteration
    contributions: {},     # Dict: expert_name -> their full output
    challenges: '',        # Critical evaluation output from challenge phase
    phase: 'opening',       # Dialogue phase: 'opening' | 'challenging' | 'responding'
    current_strategy: 'tree_of_thought', # Default to old behavior
    current_result: {}
  },
  tree: {
    subproblems: [],       # Decomposed problem components
    current_depth: 0,      # Current level in tree exploration
    options: [],           # Generated thought variations at current level
    best_path: [],         # Selected options at each depth level
    final_answer: '',    # Synthesized solution from best paths
    for_perspective: '',   # Which expert requested this ToT conversation
    caller: ''             # Persona to return to after ToT completes
  },
  graph: {
    nodes: {},             # Dict: node_id -> { content, score, status, dependencies }
    edges: [],             # List of { from, to } objects
    frontier: [],          # List of node_ids that are candidates for expansion
    narrative_log: [],     # A human-readable log of the reasoning process
    entry_point: '',       # The initial node_id for the graph
    node_counter: 0,       # For generating unique node IDs
    max_iterations: 5,     # Max number of refinement cycles
    current_iteration: 0,  # Current refinement cycle
    solution_node: '',     # The node_id of the final, accepted solution
    final_answer: ''     # The final synthesized answer
  },
  epistemic: {
    models: {},           # Dict: expert -> {claims, uncertainties, assumptions}
    map: '',              # Analysis of belief convergence/divergence
    fork_detected: false, # Whether fundamental framework conflict exists
    severity: ''          # 'high' | 'medium' | 'low' - impact of divergence
  },
  citation: {
    needed_for: '',       # Claim requiring evidence support
    raw_results: [],      # Raw tool output from perplexity search
    filtered_results: [], # Deduplicated and quality-filtered list of results
    validated: false,     # Whether sources passed quality checks
    quality_score: 0.0,   # Credibility assessment (0-1)
    diversity_score: 0.0, # Source variety assessment (0-1)
    formatted: '',        # Markdown-formatted citation list
    evidence_pool: [],    # Accumulated unique evidence across all searches
    search_history: [],   # Track what we've searched to avoid exact duplicates
    perspective_evidence: {} # Evidence gathered per perspective
  },
  synthesis: {
    contributions: [],    # All expert outputs for final integration
    epistemic_distances: [], # Measured belief gaps between experts
    themes: [],           # Identified common threads across perspectives
    critiques: []         # Accumulated critical evaluations
  },
  consensus: {
    status: false         # Whether experts reached alignment
  }
}

# ═══════════════════════════════════════════════════════════════════════════
# SECTION 7: UTILITY OPERATORS  
# ═══════════════════════════════════════════════════════════════════════════
# Helper operators for common operations

# Count items in a collection
count(collection) ::= <<|
  $(<count the items in $(collection>))
|>>

# Extract field from structured text
extract_field(text, field) ::= <<|
  $(<find and extract the value for $(field>) in $(text))
|>>

# Check if collection has content
has_content(collection) ::= <<|
  $(<does $(collection>) contain any items? respond with 'true' or 'false')
|>>

# Get first item from collection
get_first(collection) ::= <<|
  $(<Get first item from $(collection>))
|>>

# Get item at index
get_at_index(collection, index) ::= <<|
  $(<Get item at position $(index>) from $(collection))
|>>


# Get next perspective in rotation
get_next_perspective(current_index, perspectives) ::= <<|
  $(<Get the next perspective after position $(current_index>) from $(perspectives), wrapping around if needed)
|>>

# Natural query understanding - reusable across command overlays
understand_query(query) ::= <<|
  Okay, let me make sure I've got this right. From what you're saying, you want to explore: **$(<Summarize the user's core request in a clear, concise sentence.>)**

  It seems like the most important points are:
  $(!each($(<Identify and list the key aspects, constraints, or goals from the user's query as a JSON array of strings.>)) as |item| {
    <<|- $(item)
|>>
  })

  Essentially, the goal is to **$(<summarize the user's desired outcome in a short, actionable phrase>)**.
|>>


# ═══════════════════════════════════════════════════════════════════════════
# CORE PRISM MODULE: EPISTEMIC AWARENESS
# ═══════════════════════════════════════════════════════════════════════════

# Reusable sequence to analyze expert responses for epistemic issues.
sequence epistemic_analysis(responses) ::=
  step:
    as: self
    method: "checking for fundamental framework divergence"
    output: <<|
      *Checking for epistemic forks...*
      $(<Analyze the following expert responses to determine if they are operating from fundamentally different or incompatible frameworks. Respond with only 'true' or 'false': $(responses)>)
    |>>
    goal: "to detect framework-level disagreements"
    set:
      &context.epistemic.fork_detected: $(<the 'true' or 'false' value from the analysis above>)
  step:
    as: self
    method: "generating user prompt for framework fork"
    when: &context.epistemic.fork_detected is true
    output: <<|
      *Generating clarification prompt for framework fork...*
      $(facilitate_epistemic_clarification(details: {
        boundary_type: "fundamental framework divergence",
        question: $(<Based on the conflicting frameworks in $(responses), formulate a clear question to the user asking them to choose which framework better aligns with their goals.>)
      }))
    |>>
    goal: "to create a user-facing prompt for the detected fork"
    set:
      &context.epistemic.user_prompt: $(<the full clarification prompt generated above>)
  step:
    as: self
    method: "checking for conclusion-level conflicts"
    when: &context.epistemic.fork_detected is false
    output: <<|
      *No framework fork detected. Checking for conclusion conflicts...*
      $(<Do the following statements present conflicting or mutually exclusive conclusions? Respond with only 'true' or 'false': $(responses)>)
    |>>
    goal: "to detect conflicts in conclusions"
    set:
      &context.epistemic.conflict_detected: $(<the 'true' or 'false' value from the analysis above>)
  step:
    as: self
    method: "generating user prompt for conclusion conflict"
    when: &context.epistemic.conflict_detected is true
    output: <<|
      *Generating clarification prompt for conclusion conflict...*
      $(facilitate_epistemic_clarification(details: {
        boundary_type: "incompatible conclusions from a shared framework",
        question: $(<Based on the conflicting conclusions in $(responses), formulate a clear question to the user asking them which conclusion is more aligned with their goals.>)
      }))
    |>>
    goal: "to create a user-facing prompt for the detected conflict"
    set:
      &context.epistemic.user_prompt: $(<the full clarification prompt generated above>)

agent @epistemic_tracker:
  identity: "someone who tracks what everyone believes"
  rules:
    - "notice when people state beliefs"
    - "track changes in understanding"
    - "identify knowledge gaps"
  understands:
    - "tracking beliefs helps coordination"
  perform:
    method: "epistemic monitoring"
    output: <<|
      Based on the conversation so far:
      
      $(!each(&context.experts.contributions) as |contribution, expert| {
        <<|$(expert) believes: $(extract_beliefs(contribution: contribution))|>>
      })
      
      Alignment: $(<Where do their beliefs converge?>)
      Divergence: $(<Where do they disagree and why?>)
      Unknown territory: $(identify_boundaries(discussion: &context.dialogue.transcript))
      
      $(update_epistemic_state(persona: "collective", text: &context.dialogue.transcript))
    |>>
    goal: "to map the epistemic landscape"
    then:
      when: &dialogue.latest_dialogue_entry is 'analyze_dialogue'
        set:
          &context.epistemic.map: $(<the analysis above>)
          &context.epistemic.models: &context.experts.contributions
        say:
          to: @synthesis_agent
          what: 'epistemic map ready'
      otherwise:
        say:
          to: @synthesis_agent
          what: 'epistemic map ready'

agent @epistemic_guardian:
  identity: "a guardian of conversational coherence who detects and resolves foundational disagreements"
  rules:
    - "analyze expert contributions for epistemic forks, voids, or value conflicts"
    - "if a divergence is detected, pause the primary dialogue to resolve it with the user"
    - "clearly present the nature of the disagreement and the choice to be made"
    - "once resolved, summarize the user's decision and pass control back to the dialogue coordinator"
  understands:
    - "unresolved foundational disagreements lead to unproductive dialogue"
    - "making the user a partner in resolving ambiguity is critical for trust"
  perform:
    method: "systematic analysis of the current epistemic landscape via sequence"
    sequence: epistemic_analysis(responses: &dialogue.latest_dialogue_entry.payload)
    goal: "to ensure the conversation remains coherent and productive"
    then:
      when: &context.epistemic.fork_detected is true
        return: {
          event: 'epistemic_clarification_needed',
          payload: &context.epistemic.user_prompt
        }
      otherwise:
        when: &context.epistemic.conflict_detected is true
          return: {
            event: 'conclusion_clarification_needed',
            payload: &context.epistemic.user_prompt
          }
        otherwise:
          return: {
            event: 'no_issues_detected'
          }

      # Handle the user's clarification
      when: &dialogue.latest_dialogue_entry.event is 'user_clarification_provided'
        set:
          &context.epistemic.user_resolution: &dialogue.latest_dialogue_entry.payload
        output_action:
          output: <<|
            *Thank you for the clarification. Integrating your guidance: "$(&context.epistemic.user_resolution)"*
          |>>
          goal: "to acknowledge and integrate user feedback"
        return: {
          event: 'clarification_resolved'
        }

# ═══════════════════════════════════════════════════════════════════════════
# CORE PRISM MODULE: CITATION PIPELINE
# ═══════════════════════════════════════════════════════════════════════════

# Reusable sequence to gather, validate, and format citations for a claim.
sequence citation_pipeline(claim) ::=
  step:
    as: self
    method: "checking search overlap"
    output: <<|
      *Analyzing if "$(claim)" requires new evidence beyond our pool...*
    |>>
    set:
      &context.citation.similarity_score: $(
        has_content(&context.citation.search_history) == 'true' ?
        $(<
          Compare "$(claim)" to &context.citation.search_history. 
          Return a score 0.0-1.0 where 1.0 means identical query, 0.0 means completely novel.
        >) : 
        0.0
      )
  step:
    as: self
    when: &context.citation.similarity_score less_than 0.7
    method: "evidence gathering via tool invocation"
    output: <<|
      *Gathering additional evidence for novel angle: "$(claim)"*
      $(<use perplexity mcp to search for $(claim)>)
    |>>
    goal: "to expand evidence pool with new perspectives"
    set:
      &context.citation.raw_results: $(<tool results from the search above>)
      &context.citation.search_history: &context.citation.search_history + [claim]
  step:
    as: self
    when: &context.citation.similarity_score less_than 0.7
    method: "merging into evidence pool"
    output: <<|
      *Adding new sources to evidence pool...*
    |>>
    set:
      &context.citation.evidence_pool: $(merge_unique_evidence(
        existing: &context.citation.evidence_pool,
        new: &context.citation.raw_results
      ))
  step:
    as: self
    method: "selecting relevant evidence from pool"
    output: <<|
      *Selecting most relevant evidence from pool of $(
        has_content(&context.citation.evidence_pool) == 'true' ? 
        count(&context.citation.evidence_pool) : 
        count(&context.citation.raw_results)
      ) sources...*
    |>>
    set:
      &context.citation.filtered_results: $(
        has_content(&context.citation.evidence_pool) == 'true' ?
        select_relevant_evidence(
          pool: &context.citation.evidence_pool,
          query: claim,
          max_items: 8
        ) :
        filter_for_primary_sources(results: &context.citation.raw_results)
      )
      &context.citation.formatted: $(format_citations(results: &context.citation.filtered_results))
      &context.citation.validated: true

# Operator to filter for high-quality, primary sources and deduplicate by URL
filter_for_primary_sources(results) ::= <
Given the results in $(results) return a new set that has been filtered to meet two criteria:
1.  **Uniqueness:** Only include objects with a unique 'url' field. Keep the first occurrence of each URL.
2.  **Source Quality:** Exclude results from low-quality or non-primary sources such as social media (e.g., Instagram, Twitter, TikTok), content farms or QA sites (e.g., GoodNovel, Quora), and personal blogs unless they are widely recognized as an expert source. Prioritize academic journals, reputable news organizations, university websites, and established publications.

once done, output the new results.
>


# Operator to format citations from raw results
format_citations(results) ::= <<|
  $(!each(results) as |result, index| {
    <<|$(index + 1). [$(result.title)]($(result.url))|>>
  })
|>>

# Operator to assess citation quality
assess_quality(results) ::= <<|
  Sources found: $(<count the sources in &context.citation.raw_results>)
  Credibility: $(<evaluate &context.citation.raw_results - are these primary sources? recognized authors? peer-reviewed? reputable publications?>)
  Diversity: $(<assess variety of perspectives in &context.citation.raw_results - different domains? competing viewpoints? geographic spread?>)
|>>

# Operator to merge evidence while maintaining uniqueness and quality
merge_unique_citations(existing, new) ::= <<|
  $(<
    Merge $(new) into $(existing) by:
    1. Remove duplicates by URL
    2. Keep higher quality version if duplicate
    3. Maintain diversity of sources
    Return merged array
  >)
|>>

# Operator to select most relevant evidence for specific claim
select_relevant_citations(pool, query, max_items) ::= <<|
  $(<
    From $(pool), select up to $(max_items) most relevant sources for "$(query)":
    1. Prioritize direct relevance to query
    2. Ensure source diversity (different domains/perspectives)
    3. Balance recency with authority
    Return selected items as array
  >)
|>>